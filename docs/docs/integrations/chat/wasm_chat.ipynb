{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasm Chat\n",
    "\n",
    "`Wasm-chat` allows you to chat with LLMs of [GGUF](https://github.com/ggerganov/llama.cpp/blob/master/gguf-py/README.md) format both locally and via chat service.\n",
    "\n",
    "\n",
    "`Wasm-chat` is driven by [WasmEdge Runtime](https://wasmedge.org/), a popular WebAssmbly runtime. It is also an attempt to run AI tasks within wasm containers.\n",
    "\n",
    "## Chat Locally\n",
    "\n",
    "### Setup\n",
    "\n",
    "To deploy WasmEdge Runtime and `wasm-infer.wasm` in your local system, run the following command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!curl -sSf https://raw.githubusercontent.com/second-state/wasm-llm/main/deploy.sh | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model\n",
    "\n",
    "For the purpose of demonstration, we use a smaller model `TinyLlama-1.1B-Chat-v0.3`. Run the following command to download the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!curl -LO https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q5_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More models and sample code can be found in [GGUF Models](https://github.com/second-state/wasm-llm/blob/main/MODEL.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: One-turn Conversation\n",
    "\n",
    "The following code shows a one-turn conversation with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.wasm_chat import WasmChatLocal, PromptTemplateType\n",
    "from langchain.schema.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "model_file = \"tinyllama-1.1b-chat-v0.3.Q5_K_M.gguf\"\n",
    "\n",
    "chat = WasmChatLocal(model_file=model_file, prompt_template=PromptTemplateType.ChatML)\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an AI assistant\")\n",
    "user_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "messages = [system_message, user_message]\n",
    "chat_result = chat(messages)\n",
    "print(f\"[Bot] {chat_result.content}\")\n",
    "\n",
    "assert isinstance(chat_result, AIMessage)\n",
    "assert isinstance(chat_result.content, str)\n",
    "assert \"Paris\" in chat_result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat via API Service\n",
    "\n",
    "Compare with chat locally, chat via API service is more convenient. You don't need to install WasmEdge Runtime and download the model. You can chat with the model by sending a HTTP request to the API service.\n",
    "\n",
    "More importantly, by following the steps in [README](https://github.com/second-state/llama-utils/tree/main/api-server#readme), you can host your own API service so that you can chat with any models you like on any device you have anywhere as long as the internet is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.wasm_chat import WasmChatService\n",
    "from langchain.schema.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "ip_addr = \"<service ip address>\"\n",
    "port = \"<service port>\"\n",
    "\n",
    "chat = WasmChatService(service_ip_addr=ip_addr, service_port=port)\n",
    "system_message = SystemMessage(content=\"You are an AI assistant\")\n",
    "user_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "messages = [system_message, user_message]\n",
    "response = chat(messages)\n",
    "\n",
    "assert isinstance(chat_result, AIMessage)\n",
    "assert isinstance(chat_result.content, str)\n",
    "assert \"Paris\" in chat_result.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasm Chat\n",
    "\n",
    "`Wasm-chat` allows you to chat with LLMs of [GGUF](https://github.com/ggerganov/llama.cpp/blob/master/gguf-py/README.md) format both locally and via chat service.\n",
    "\n",
    "\n",
    "`Wasm-chat` is driven by [WasmEdge Runtime](https://wasmedge.org/), a popular WebAssmbly runtime. It is also an attempt to run AI tasks within wasm containers.\n",
    "\n",
    "## Chat Locally\n",
    "\n",
    "### Setup\n",
    "\n",
    "To deploy WasmEdge Runtime and `wasm-infer.wasm` in your local system, run the following command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The installation will deploy 'WasmEdge Runtime' and 'wasm-infer.wasm' in your local environment:\n",
      "\n",
      "[+] Checking the operating system ...\n",
      "\n",
      "[+] Checking if 'git' and 'curl' are installed ...\n",
      "\n",
      "[+] Installing WasmEdge ...\n",
      "\n",
      "\u001b[0;33mUsing Python: /home/ubuntu/miniconda3/envs/langchain/bin/python3 \u001b[0m\n",
      "ERROR   - Exception on process - rc= 127 output= b'' command= ['/usr/local/cuda/bin/nvcc --version 2>/dev/null']\n",
      "WARNING - Experimental Option Selected: plugins\n",
      "WARNING - plugins option may change later\n",
      "INFO    - Compatible with current configuration\n",
      "INFO    - Running Uninstaller\n",
      "WARNING - SHELL variable not found. Using bash as SHELL\n",
      "INFO    - shell configuration updated\n",
      "INFO    - Downloading WasmEdge\n",
      "|============================================================|100.00 %INFO    - Downloaded\n",
      "INFO    - Installing WasmEdge\n",
      "INFO    - WasmEdge Successfully installed\n",
      "INFO    - Downloading Plugin: wasi_nn-ggml\n",
      "|============================================================|100.00 %INFO    - Downloaded\n",
      "INFO    - Run:\n",
      "source /home/ubuntu/.bashrc\n",
      "\n",
      "    The WasmEdge Runtime 0.13.5 is installed in /home/ubuntu/.wasmedge/bin/wasmedge.\n",
      "\n",
      "\n",
      "[+] Downloading 'wasm-infer.wasm' ...\n",
      "\n",
      "######################################################################## 100.0%\n",
      "\n",
      "* The installation is done! To uninstall WasmEdge Runtime, use the command 'bash deploy.sh uninstall'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl -sSf https://raw.githubusercontent.com/second-state/wasm-llm/main/deploy.sh | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model\n",
    "\n",
    "For the purpose of demonstration, we use a smaller model `TinyLlama-1.1B-Chat-v0.3`. Run the following command to download the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1176  100  1176    0     0   5909      0 --:--:-- --:--:-- --:--:--  5939\n",
      "100  745M  100  745M    0     0   232M      0  0:00:03  0:00:03 --:--:--  272M\n"
     ]
    }
   ],
   "source": [
    "!curl -LO https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q5_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More models and sample code can be found in [GGUF Models](https://github.com/second-state/wasm-llm/blob/main/MODEL.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: One-turn Conversation\n",
    "\n",
    "The following code shows a one-turn conversation with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bot] Paris\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models.wasm_chat import ChatWasmLocal, PromptTemplateType\n",
    "from langchain.schema.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "model_file = \"/Volumes/Store/models/gguf/tinyllama-1.1b-chat-v0.3.Q5_K_M.gguf\"\n",
    "\n",
    "chat = ChatWasmLocal(model_file=model_file, prompt_template=PromptTemplateType.ChatML)\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an AI assistant\")\n",
    "user_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "messages = [system_message, user_message]\n",
    "chat_result = chat(messages)\n",
    "print(f\"[Bot] {chat_result.content}\")\n",
    "\n",
    "assert isinstance(chat_result, AIMessage)\n",
    "assert isinstance(chat_result.content, str)\n",
    "assert \"Paris\" in chat_result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat via API Service\n",
    "\n",
    "Compare with chat locally, chat via API service is more convenient. You don't need to install WasmEdge Runtime and download the model. You can chat with the model by sending a HTTP request to the API service.\n",
    "\n",
    "More importantly, by following the steps in [README](https://github.com/second-state/llama-utils/tree/main/api-server#readme), you can host your own API service so that you can chat with any models you like on any device you have anywhere as long as the internet is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error code: 502, reason: Bad Gateway",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/Dev/secondstate/me/langchain/docs/docs/integrations/chat/wasm_chat.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/Dev/secondstate/me/langchain/docs/docs/integrations/chat/wasm_chat.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m user_message \u001b[39m=\u001b[39m HumanMessage(content\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhat is the capital of France?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/Dev/secondstate/me/langchain/docs/docs/integrations/chat/wasm_chat.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m messages \u001b[39m=\u001b[39m [system_message, user_message]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/Dev/secondstate/me/langchain/docs/docs/integrations/chat/wasm_chat.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m response \u001b[39m=\u001b[39m chat(messages)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/Dev/secondstate/me/langchain/docs/docs/integrations/chat/wasm_chat.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Bot] \u001b[39m\u001b[39m{\u001b[39;00mchat_result\u001b[39m.\u001b[39mcontent\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/Dev/secondstate/me/langchain/docs/docs/integrations/chat/wasm_chat.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(chat_result, AIMessage)\n",
      "File \u001b[0;32m/Volumes/Dev/secondstate/me/langchain/libs/langchain/langchain/chat_models/base.py:600\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    594\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    595\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    599\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 600\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    601\u001b[0m         [messages], stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    602\u001b[0m     )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    603\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    604\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m/Volumes/Dev/secondstate/me/langchain/libs/langchain/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    348\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 349\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    350\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    351\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    352\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    353\u001b[0m ]\n\u001b[1;32m    354\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m/Volumes/Dev/secondstate/me/langchain/libs/langchain/langchain/chat_models/base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    337\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 339\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    340\u001b[0m                 m,\n\u001b[1;32m    341\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    342\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    343\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/Volumes/Dev/secondstate/me/langchain/libs/langchain/langchain/chat_models/base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    493\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Volumes/Dev/secondstate/me/langchain/libs/langchain/langchain/chat_models/wasm_chat.py:229\u001b[0m, in \u001b[0;36mChatWasmService._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chat(messages, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m res\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError code: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m, reason: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m.\u001b[39mreason\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m response \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mjson()\n\u001b[1;32m    233\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "\u001b[0;31mValueError\u001b[0m: Error code: 502, reason: Bad Gateway"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models.wasm_chat import ChatWasmService\n",
    "from langchain.schema.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "ip_addr = \"0.0.0.0\" # change it to your own service ip address\n",
    "port = \"8080\" # change it to your own service port\n",
    "\n",
    "chat = ChatWasmService(service_ip_addr=ip_addr, service_port=port)\n",
    "system_message = SystemMessage(content=\"You are an AI assistant\")\n",
    "user_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "messages = [system_message, user_message]\n",
    "response = chat(messages)\n",
    "print(f\"[Bot] {chat_result.content}\")\n",
    "\n",
    "assert isinstance(chat_result, AIMessage)\n",
    "assert isinstance(chat_result.content, str)\n",
    "assert \"Paris\" in chat_result.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

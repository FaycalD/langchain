{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasm Chat\n",
    "\n",
    "`Wasm-chat` allows you to chat with LLMs of [GGUF](https://github.com/ggerganov/llama.cpp/blob/master/gguf-py/README.md) format both locally and via chat service.\n",
    "\n",
    "\n",
    "`Wasm-chat` is driven by [WasmEdge Runtime](https://wasmedge.org/), a popular WebAssmbly runtime. It is also an attempt to run AI tasks within wasm containers.\n",
    "\n",
    "## Chat Locally\n",
    "\n",
    "### Setup\n",
    "\n",
    "To deploy WasmEdge Runtime and `wasm-infer.wasm` in your local system, run the following command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The installation will deploy 'WasmEdge Runtime' and 'wasm-infer.wasm' in your local environment:\n",
      "\n",
      "[+] Checking the operating system ...\n",
      "\n",
      "[+] Checking if 'git' and 'curl' are installed ...\n",
      "\n",
      "[+] Installing WasmEdge ...\n",
      "\n",
      "\u001b[0;33mUsing Python: /home/ubuntu/miniconda3/envs/langchain/bin/python3 \u001b[0m\n",
      "ERROR   - Exception on process - rc= 127 output= b'' command= ['/usr/local/cuda/bin/nvcc --version 2>/dev/null']\n",
      "WARNING - Experimental Option Selected: plugins\n",
      "WARNING - plugins option may change later\n",
      "INFO    - Compatible with current configuration\n",
      "INFO    - Running Uninstaller\n",
      "WARNING - SHELL variable not found. Using bash as SHELL\n",
      "INFO    - shell configuration updated\n",
      "INFO    - Downloading WasmEdge\n",
      "|============================================================|100.00 %INFO    - Downloaded\n",
      "INFO    - Installing WasmEdge\n",
      "INFO    - WasmEdge Successfully installed\n",
      "INFO    - Downloading Plugin: wasi_nn-ggml\n",
      "|============================================================|100.00 %INFO    - Downloaded\n",
      "INFO    - Run:\n",
      "source /home/ubuntu/.bashrc\n",
      "\n",
      "    The WasmEdge Runtime 0.13.5 is installed in /home/ubuntu/.wasmedge/bin/wasmedge.\n",
      "\n",
      "\n",
      "[+] Downloading 'wasm-infer.wasm' ...\n",
      "\n",
      "######################################################################## 100.0%\n",
      "\n",
      "* The installation is done! To uninstall WasmEdge Runtime, use the command 'bash deploy.sh uninstall'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl -sSf https://raw.githubusercontent.com/second-state/wasm-llm/main/deploy.sh | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model\n",
    "\n",
    "For the purpose of demonstration, we use a smaller model `TinyLlama-1.1B-Chat-v0.3`. Run the following command to download the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1176  100  1176    0     0   5909      0 --:--:-- --:--:-- --:--:--  5939\n",
      "100  745M  100  745M    0     0   232M      0  0:00:03  0:00:03 --:--:--  272M\n"
     ]
    }
   ],
   "source": [
    "!curl -LO https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q5_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More models and sample code can be found in [GGUF Models](https://github.com/second-state/wasm-llm/blob/main/MODEL.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: One-turn Conversation\n",
    "\n",
    "The following code shows a one-turn conversation with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bot] Paris\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models.wasm_chat import WasmChatLocal, PromptTemplateType\n",
    "from langchain.schema.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "model_file = \"tinyllama-1.1b-chat-v0.3.Q5_K_M.gguf\"\n",
    "\n",
    "chat = WasmChatLocal(model_file=model_file, prompt_template=PromptTemplateType.ChatML)\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an AI assistant\")\n",
    "user_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "messages = [system_message, user_message]\n",
    "chat_result = chat(messages)\n",
    "print(f\"[Bot] {chat_result.content}\")\n",
    "\n",
    "assert isinstance(chat_result, AIMessage)\n",
    "assert isinstance(chat_result.content, str)\n",
    "assert \"Paris\" in chat_result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat via API Service\n",
    "\n",
    "Compare with chat locally, chat via API service is more convenient. You don't need to install WasmEdge Runtime and download the model. You can chat with the model by sending a HTTP request to the API service.\n",
    "\n",
    "More importantly, by following the steps in [README](https://github.com/second-state/llama-utils/tree/main/api-server#readme), you can host your own API service so that you can chat with any models you like on any device you have anywhere as long as the internet is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bot] Paris\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models.wasm_chat import WasmChatService\n",
    "from langchain.schema.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "ip_addr = \"0.0.0.0\" # change it to your own service ip address\n",
    "port = \"8080\" # change it to your own service port\n",
    "\n",
    "chat = WasmChatService(service_ip_addr=ip_addr, service_port=port)\n",
    "system_message = SystemMessage(content=\"You are an AI assistant\")\n",
    "user_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "messages = [system_message, user_message]\n",
    "response = chat(messages)\n",
    "print(f\"[Bot] {chat_result.content}\")\n",
    "\n",
    "assert isinstance(chat_result, AIMessage)\n",
    "assert isinstance(chat_result.content, str)\n",
    "assert \"Paris\" in chat_result.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
